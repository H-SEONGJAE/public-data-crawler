from playwright.sync_api import sync_playwright
import os, re, time
import shutil
import os
os.environ["PLAYWRIGHT_BROWSERS_PATH"] = "0"

def main(inst_name, org_url):
    def clean_title(text):
        text = text.strip()
        text = re.sub(r"[\\/:*?\"<>|]", "_", text)
        text = re.sub(r"\s+", " ", text)
        return text
    
    
    # ğŸ”½ load_items â€“ ë Œë”ë§ ì§€ì—° ë¬¸ì œ í•´ê²° ë²„ì „
    def load_items(page):
        page.wait_for_selector("div.result-list ul li")
    
        # ğŸ”½ li ê°œìˆ˜ ì•ˆì •í™” (ìµœëŒ€ 1ì´ˆ)
        last_count = -1
        stable_round = 0
        max_wait = 10  # 0.1ì´ˆ Ã— 10 = 1ì´ˆ
    
        for _ in range(max_wait):
            items = page.query_selector_all("div.result-list ul li")
            count = len(items)
    
            if count == last_count:
                stable_round += 1
                if stable_round >= 3:  # 3ë²ˆ ì—°ì† ë™ì¼ â†’ ì•ˆì •ë¨
                    break
            else:
                stable_round = 0
    
            last_count = count
            time.sleep(0.1)
    
        # ğŸ”½ ì•ˆì •ëœ items ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        datasets = []
        for li in items:
            a = li.query_selector("a[href*='/data/']")
            if not a:
                continue
    
            title_el = li.query_selector("span.title")
            raw_title = title_el.inner_text().strip() if title_el else a.inner_text().strip()
    
            href = a.get_attribute("href")
            if href.startswith("/"):
                href = "https://www.data.go.kr" + href
    
            datasets.append({"title": clean_title(raw_title), "href": href})
    
        return datasets
    
    
    # ğŸ”½ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜
    def goto_next(page):
        page.wait_for_selector("nav.pagination strong.active")
    
        curr = page.query_selector("nav.pagination strong.active")
        if not curr:
            return False
    
        next_el = curr.evaluate_handle("node => node.nextElementSibling")
        if not next_el:
            return False
    
        tag = next_el.get_property("tagName").json_value().lower()
        if tag != "a":
            return False
    
        page.evaluate("(el)=>el.click()", next_el)
        page.wait_for_load_state("networkidle")
        time.sleep(1)
        return True
    
    
    # ğŸ”½ MAIN
    def run_crawler(inst_name, org_url):
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(accept_downloads=True)
            page = context.new_page()
    
            ROOT_DIR = f"{inst_name}_í¬í„¸ë°ì´í„°"
            os.makedirs(ROOT_DIR, exist_ok=True)
    
            page.goto(org_url, wait_until="domcontentloaded")
    
            page.wait_for_load_state("networkidle")
            time.sleep(3)
    
            print(f"ğŸ¢ {inst_name} ê¸°ê´€ë³„ ì „ìš© í˜ì´ì§€ ì ‘ì† ì™„ë£Œ")
    
            page.wait_for_selector("a:has-text('íŒŒì¼ë°ì´í„°')")
            page.click("a:has-text('íŒŒì¼ë°ì´í„°')")
            page.wait_for_selector("div.result-list ul li")
    
            page_num = 1
    
    
            while True:
                print("\n============================")
                print(f"ğŸ“„ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹œì‘")
                print("============================")
    
                # ğŸ”½ ì•ˆì •í™”ëœ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                datasets = load_items(page)
                print(f"ğŸ“‘ {len(datasets)}ê°œ ë°ì´í„°ì…‹ ë°œê²¬")
    
                # ğŸ”½ ë°ì´í„°ì…‹ ìˆœíšŒ
                for idx, d in enumerate(datasets, start=1):
                    title = d["title"]
                    href = d["href"]
    
                    print(f"\nğŸ“‚ [{idx}] {title}")
                    print(f"ğŸ”— {href}")
    
                    save_dir = os.path.join(ROOT_DIR, title)
                    os.makedirs(save_dir, exist_ok=True)
    
                    past_dir = os.path.join(save_dir, "ê³¼ê±°ë°ì´í„°")
                    os.makedirs(past_dir, exist_ok=True)
    
                    # ìƒì„¸ í˜ì´ì§€ ì´ë™
                    page.goto(href)
                    page.wait_for_load_state("networkidle")
                    time.sleep(0.4)
    
                    # ğŸ”½ í˜„ì¬ë°ì´í„° ë‹¤ìš´ë¡œë“œ
                    try:
                        with page.expect_download(timeout=40000) as dl_info:
                            page.click("a:has-text('ë‹¤ìš´ë¡œë“œ')")
                        dl = dl_info.value
                        original = dl.suggested_filename
                        dl.save_as(os.path.join(save_dir, original))
                        print(f"   âœ… í˜„ì¬ë°ì´í„° ì €ì¥ë¨ â†’ {original}")
                    except Exception as e:
                        print("   âš  í˜„ì¬ë°ì´í„° ì‹¤íŒ¨:", e)
    
                    # ğŸ”½ ê³¼ê±°ë°ì´í„° ë‹¤ìš´ë¡œë“œ
                    try:
                        links = page.query_selector_all("a[onclick*='fileDataDetail']")
                        print(f"ğŸ“‚ ê³¼ê±°ë°ì´í„° {len(links)}ê±´")
    
                        for j, el in enumerate(links, start=1):
                            onclick = el.get_attribute("onclick")
                            page.evaluate(onclick)
    
                            # ğŸ”½ ëª¨ë‹¬ ë¡œë”© ëŒ€ê¸°
                            page.wait_for_function("""
                                ()=> {
                                    const m=document.querySelector('#layer_data_infomation .file-meta-table-mobile');
                                    return m && window.getComputedStyle(m).display==='block';
                                }
                            """, timeout=7000)
    
                            modal = page.query_selector("#layer_data_infomation .file-meta-table-mobile")
    
                            # ğŸ”½ CSV ë²„íŠ¼ ìš°ì„ 
                            csv_btns = modal.query_selector_all("a.button.white:has-text('CSV')")
    
                            if csv_btns:
                                target_btn = csv_btns[-1]
                            else:
                                # ğŸ”½ CSV ì—†ìŒ â†’ ì²« ë²ˆì§¸ ë²„íŠ¼ fallback
                                fallback = modal.query_selector_all("a.button.white")
                                if not fallback:
                                    print("   âš  ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì—†ìŒ â†’ íŒ¨ìŠ¤")
                                    close = page.query_selector("#layer_data_infomation button.close")
                                    if close:
                                        close.click()
                                    continue
                                target_btn = fallback[0]
                                print("   â†’ CSV ì—†ìŒ â†’ ì²« ë²ˆì§¸ ë²„íŠ¼ ì‚¬ìš©")
    
                            # ğŸ”½ ë‹¤ìš´ë¡œë“œ
                            with page.expect_download(timeout=60000) as d2:
                                page.evaluate("(el)=>el.click()", target_btn)
                            file = d2.value
    
                            original = file.suggested_filename
                            base, ext = os.path.splitext(original)
                            new_name = f"{base}(ê³¼ê±°{j}){ext}"
    
                            file.save_as(os.path.join(past_dir, new_name))
                            print(f"   âœ… ê³¼ê±°ë°ì´í„°[{j}] ì €ì¥ë¨ â†’ {new_name}")
    
                            # ğŸ”½ ëª¨ë‹¬ ë‹«ê¸°
                            close = page.query_selector("#layer_data_infomation button.close")
                            if close:
                                close.click()
    
                    except Exception as e:
                        print("   âš  ê³¼ê±°ë°ì´í„° ì˜¤ë¥˜:", e)
    
                    # ğŸ”½ ëª©ë¡ìœ¼ë¡œ ë³µê·€
                    page.go_back()
                    page.wait_for_load_state("networkidle")
                    time.sleep(0.4)
    
                # ğŸ”½ ë‹¤ìŒ í˜ì´ì§€ ì´ë™
                if not goto_next(page):
                    print("\nğŸ“Œ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ â†’ ì¢…ë£Œ")
                    break
    
                page_num += 1
    
            print("\nğŸ‰ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            browser.close()
            
            zip_path = shutil.make_archive(ROOT_DIR, "zip", ROOT_DIR)
            return zip_path
    return run_crawler(inst_name, org_url)

if __name__ == "__main__":
    import json
    with open("config.json", "r", encoding="utf-8") as f:
        config = json.load(f)

    main(config["inst_name"], config["org_url"])
